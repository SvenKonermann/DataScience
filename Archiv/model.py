# -*- coding: utf-8 -*-
"""Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1StUK0SL9RXrg_neiA-CNffhriqYoEXgG

## Machine Learning Pipeline: Wrapping up for Deployment


In the previous notebooks, we worked through the typical Machine Learning pipeline steps to build a regression model that allows us to predict house prices. Briefly, we transformed variables in the dataset to make them suitable for use in a Regression model, then we selected the most predictive variables and finally we trained our model.

Now, we want to deploy our model. We want to create an API, which we can call with new data, with new characteristics about houses, to get an estimate of the SalePrice. In order to do so, we need to write code in a very specific way. We will show you how to write production code in the next sections.

Here, we will summarise the key pieces of code, that we need to take forward for this particular project, to put our model in production.

Let's go ahead and get started.

### Setting the seed

It is important to note, that we are engineering variables and pre-processing data with the idea of deploying the model. Therefore, from now on, for each step that includes some element of randomness, it is extremely important that we **set the seed**. This way, we can obtain reproducibility between our research and our development code.

This is perhaps one of the most important lessons that you need to take away from this course: **Always set the seeds**.

Let's go ahead and load the dataset.
"""

# to handle datasets
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import sklearn


# to divide train and test set
from sklearn.model_selection import train_test_split

# feature scaling
from sklearn.preprocessing import MinMaxScaler

# to build the models
from sklearn.linear_model import Lasso

# to evaluate the models
from sklearn.metrics import mean_squared_error, r2_score
from math import sqrt

# to persist the model and the scaler
import joblib

# to visualise al the columns in the dataframe
pd.pandas.set_option('display.max_columns', None)

import warnings
warnings.simplefilter(action='ignore')

"""## Load data

We need the training data to train our model in the production environment. 
"""

# load dataset
data = pd.read_csv('Indizes.csv')
data = data.drop(data.columns[0],axis=1)
data = data.drop(data.columns[0],axis=1)
#data = data['Eröffnung_x'.replace('.','')



#data['Eröffnung_x'] = data['Eröffnung_x'].astype("float64")
#data['Eröffnung_y'] = data['Eröffnung_y'].astype("float64")
#data = data.drop(data.columns[0],axis=1)
print(data)
#data.head()
data.info()

#data = data['Eröffnung_x'.replace('.','')



#data['Eröffnung_x'] = data['Eröffnung_x'].astype("float64")
#data['Eröffnung_y'] = data['Eröffnung_y'].astype("float64")
#data = data.drop(data.columns[0],axis=1)
print(data)
#data.head()
data.info()

import seaborn as sns
df_heatmap = data.corr().round(2)
f, ax = plt.subplots(figsize=(15, 5))
sns.heatmap(df_heatmap, annot=True, annot_kws={'size': 12}, linewidths=.5)

"""## Separate dataset into train and test"""

X_train, X_test, y_train, y_test = train_test_split(
    data,
    data['Schluss DAX'],
    test_size=0.1,
    # we are setting the seed here
    random_state=0)

X_train.shape, X_test.shape

X_train.head()

"""## Selected features"""

# load selected features
features = pd.read_csv('selected_features.csv')

# Added the extra feature, LotFrontage
features = features['0'].to_list() + ['LotFrontage']

print('Number of features: ', len(features))

"""## Engineer missing values

### Categorical variables

For categorical variables, we will replace missing values with the string "missing".
"""

# make a list of the categorical variables that contain missing values

vars_with_na = [
    var for var in features
    if X_train[var].isnull().sum() > 0 and X_train[var].dtypes == 'O'
]

# display categorical variables that we will engineer:
vars_with_na

"""Note that we have much less categorical variables with missing values than in our original dataset. But we still use categorical variables with NA for the final model, so we need to include this piece of feature engineering logic in the deployment pipeline. """

# I bring forward the code used in the feature engineering notebook:
# (step 2)

X_train[vars_with_na] = X_train[vars_with_na].fillna('Missing')
X_test[vars_with_na] = X_test[vars_with_na].fillna('Missing')

# check that we have no missing information in the engineered variables
X_train[vars_with_na].isnull().sum()

"""### Numerical variables

To engineer missing values in numerical variables, we will:

- add a binary missing value indicator variable
- and then replace the missing values in the original variable with the mode

"""

# make a list of the numerical variables that contain missing values:

vars_with_na = [
    var for var in features
    if X_train[var].isnull().sum() > 0 and X_train[var].dtypes != 'O'
]

# display numerical variables with NA
vars_with_na

# I bring forward the code used in the feature engineering notebook
# with minor adjustments (step 2):

var = 'LotFrontage'

# calculate the mode
mode_val = X_train[var].mode()[0]
print('mode of LotFrontage: {}'.format(mode_val))

# replace missing values by the mode
# (in train and test)
X_train[var] = X_train[var].fillna(mode_val)
X_test[var] = X_test[var].fillna(mode_val)

"""## Temporal variables

One of our temporal variables was selected to be used in the final model: 'YearRemodAdd'

So we need to deploy the bit of code that creates it.
"""

# create the temporal var "elapsed years"

# I bring this bit of code forward from the notebook on feature
# engineering (step 2)

def elapsed_years(df, var):
    # capture difference between year variable
    # and year in which the house was sold
    
    df[var] = df['YrSold'] - df[var]
    
    return df

X_train = elapsed_years(X_train, 'YearRemodAdd')
X_test = elapsed_years(X_test, 'YearRemodAdd')

"""### Numerical variable transformation"""

# we apply the logarithmic function to the variables that
# were selected (and the target):

for var in ['LotFrontage', '1stFlrSF', 'GrLivArea', 'SalePrice']:
    X_train[var] = np.log(X_train[var])
    X_test[var] = np.log(X_test[var])

"""## Categorical variables

### Group rare labels
"""

# let's capture the categorical variables first

cat_vars = [var for var in features if X_train[var].dtype == 'O']

cat_vars

# bringing thise from the notebook on feature engineering (step 2):

def find_frequent_labels(df, var, rare_perc):
    
    # function finds the labels that are shared by more than
    # a certain % of the houses in the dataset

    df = df.copy()

    tmp = df.groupby(var)['SalePrice'].count() / len(df)

    return tmp[tmp > rare_perc].index


for var in cat_vars:
    
    # find the frequent categories
    frequent_ls = find_frequent_labels(X_train, var, 0.01)
    print(var)
    print(frequent_ls)
    print()
    
    # replace rare categories by the string "Rare"
    X_train[var] = np.where(X_train[var].isin(
        frequent_ls), X_train[var], 'Rare')
    
    X_test[var] = np.where(X_test[var].isin(
        frequent_ls), X_test[var], 'Rare')

"""### Encoding of categorical variables

"""

# this function will assign discrete values to the strings of the variables,
# so that the smaller value corresponds to the category that shows the smaller
# mean house sale price


def replace_categories(train, test, var, target):

    # order the categories in a variable from that with the lowest
    # house sale price, to that with the highest
    ordered_labels = train.groupby([var])[target].mean().sort_values().index

    # create a dictionary of ordered categories to integer values
    ordinal_label = {k: i for i, k in enumerate(ordered_labels, 0)}

    # use the dictionary to replace the categorical strings by integers
    train[var] = train[var].map(ordinal_label)
    test[var] = test[var].map(ordinal_label)
    
    print(var)
    print(ordinal_label)
    print()

for var in cat_vars:
    replace_categories(X_train, X_test, var, 'SalePrice')

# check absence of na
[var for var in features if X_train[var].isnull().sum() > 0]

# check absence of na
[var for var in features if X_test[var].isnull().sum() > 0]

"""### Feature Scaling

For use in linear models, features need to be either scaled or normalised. In the next section, I will scale features between the min and max values:
"""

# capture the target
y_train = X_train['Eröffnung_y']
y_test = X_test['Eröffnung_y']

scaler = MinMaxScaler()
scaler.fit(X_train)
IndexListe= ["DAX", "EURO STOXX 50", "Dow Jones",  "SMI", "MSCI World", "FTSE", "ATX", "SDAX", "Tec DAX", "NASDAQ 100", "Nikkei", "Hang Seng", "SHANGHAI STOCK EXCHANGE B SHARES", "Emerging Markets", "Bovespa"]
scaled = scaler.transform(X_train)
for i, col in enumerate(IndexListe):
    X_train[col] = scaled[:,i]

scaled_test = scaler.transform(X_test)

print('Vor dem Scaling:')
display(X_test.head())

for i, col in enumerate(IndexListe):
    X_test[col] = scaled_test[:,i]

print('\nNach dem Scaling:')
display(X_test.head())

# explore maximum values of variables
scaler.data_max_

# explore minimum values of variables
scaler.data_min_

# transform the train and test set, and add on the Id and SalePrice variables
X_train = scaler.transform(X_train[features])
X_test = scaler.transform(X_test[features])

"""## Train the Linear Regression: Lasso"""

# set up the model
# remember to set the random_state / seed

lin_model = Lasso(alpha=0.005, random_state=0)

# train the model
lin_model.fit(X_train, y_train)

# we persist the model for future use
#joblib.dump(lin_model, 'lasso_regression.pkl')

from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.linear_model import BayesianRidge
import xgboost as xgb
from sklearn.linear_model import Lasso
from sklearn.linear_model import LassoLars
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor

# Wir kreiren hier einen Benchmarkregressor, welcher den Durchschnittswert unseres Trainingsamples als Vorhersage nimmt.
class BenchmarkRegressor:
    def __init__(self):
        pass
    
    def fit(self, X, y, **kwargs):
        self.mean = y.mean()
        
    def predict(self, X):
        return [self.mean] * len(X)
    
    def get_params(self, deep=False):
        return {}


bm_regr = BenchmarkRegressor()
lr_regr = LinearRegression()
ri_regr = Ridge()
br_regr = BayesianRidge()
ls_regr = Lasso()
ll_regr = LassoLars()
ne_regr = KNeighborsRegressor()
dt_regr = DecisionTreeRegressor()
rf_regr = RandomForestRegressor()
xg_regr = xgb.XGBRegressor()

# Liste mit dem Namen der Modelle und dem Modell selbst, dies erwartet das Pipeline Objekt von SKLearn anschliesssend als Input.
models = [
    ('Benchmark', bm_regr),
    ('LR', lr_regr),
    ('Ridge', ri_regr),
    ('Bayesian Ridge', br_regr),
    ('Lasso', ls_regr),
    ('LARS Lasso', ll_regr),
    ('Nearest Neighbors (KNN) regression', ne_regr),
    ('Decision Tree', dt_regr),
    ('Random Forest', rf_regr),
    ('XGBoost', xg_regr),
]

from sklearn.metrics import explained_variance_score
from sklearn.metrics import max_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn.decomposition import PCA
from sklearn.pipeline import FeatureUnion, Pipeline


for name, model in models:
    pipelined_model = Pipeline([ 
                                 ('pca', PCA(n_components = 3)), 
                                 (name, model)
                            ])
    
    # Training des Models
    pipelined_model.fit(X_train, y_train)
    
    # Vorhersage des trainierten models auf X_test
    y_hat = pipelined_model.predict(X_test)
    
    # Berechnung der verschiedenen Messwerte resp. KPI's
    EVS = (explained_variance_score(y_test, y_hat))
    ME = (max_error(y_test, y_hat))
    MAE = (mean_absolute_error(y_test, y_hat))
    MSE = np.sqrt(mean_squared_error(y_test, y_hat, squared=True))
    RMSE = np.sqrt(mean_squared_error(y_test, y_hat, squared=False))
    R2 = r2_score(y_test, y_hat)
    print('Model: ', name, ' | EVS: ', EVS)
    print('Model: ', name, ' | ME: ', ME)
    print('Model: ', name, ' | MAE: ', MAE)
    print('Model: ', name, ' | MSE ', MSE)
    print('Model: ', name, ' | RMSE ', RMSE)
    print('Model: ', name, ' | R2 ', R2)
    print('----------------')

start = time.time()

from sklearn.neural_network import MLPRegressor
import time

nn_sat = MLPRegressor(hidden_layer_sizes = (15,15) ,max_iter=400).fit(X_train,y_train)
print('Satisfaction predictions')
print('R2: ', nn_sat.score(X_train,y_train))
plt.plot(nn_sat.loss_curve_)

predictions_nn_sat = nn_sat.predict(X_test)
print('R2 for predictions: ', nn_sat.score(X_test, y_test))
EVS = (explained_variance_score(y_test, y_hat))
ME = (max_error(y_test, y_hat))
MAE = (mean_absolute_error(y_test, y_hat))
MSE = np.sqrt(mean_squared_error(y_test, y_hat, squared=True))
RMSE = np.sqrt(mean_squared_error(y_test, y_hat, squared=False))
print('Model: ', name, ' | EVS: ', EVS)
print('Model: ', name, ' | ME: ', ME)
print('Model: ', name, ' | MAE: ', MAE)
print('Model: ', name, ' | MSE ', MSE)
print('Model: ', name, ' | RMSE ', RMSE)
print('----------------')

end = time.time()
print('Time: ', end - start)

std_nn = np.sqrt(sum((predictions_nn_sat - y_test)**2)/len(y_test))
print('Standard deviation: ', std_nn)

print(predictions_nn_sat)
print(y_test)

print(predictions_nn_sat[1])
print(y_test[3745])

# evaluate the model:
# ====================

# remember that we log transformed the output (SalePrice)
# in our feature engineering notebook (step 2).

# In order to get the true performance of the Lasso
# we need to transform both the target and the predictions
# back to the original house prices values.

# We will evaluate performance using the mean squared error and
# the root of the mean squared error and r2

# make predictions for train set
pred = lin_model.predict(X_train)

# determine mse and rmse
#print('train mse: {}'.format(int(
#    mean_squared_error(np.exp(y_train), np.exp(pred)))))
#print('train rmse: {}'.format(int(
#    sqrt(mean_squared_error(np.exp(y_train), np.exp(pred))))))
print('train r2: {}'.format(
    r2_score(np.exp(y_train), np.exp(pred))))
print()

# make predictions for test set
pred = lin_model.predict(X_test)

# determine mse and rmse
#print('test mse: {}'.format(int(
#    mean_squared_error(np.exp(y_test), np.exp(pred)))))
#print('test rmse: {}'.format(int(
#    sqrt(mean_squared_error(np.exp(y_test), np.exp(pred))))))
print('test r2: {}'.format(
    r2_score(np.exp(y_test), np.exp(pred))))
print()

print('Average house price: ', int(np.exp(y_train).median()))

"""That is all for this notebook. And that is all for this section too.

**In the next section, we will show you how to productionise this code for model deployment**.
"""

